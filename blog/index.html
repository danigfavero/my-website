<!DOCTYPE html>
<html>
    <head>
        <meta charset="UTF-8">
        <title>Blog</title>

        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Open+Sans:wght@400;600;700&display=swap" rel="stylesheet">        
        <link rel="stylesheet" href="../css/styles.css">
        <link rel="stylesheet" href="../css/blog.css">
        <link rel="icon" type="image/png" href="../img/favicon.png"/>
    </head>

    <body>
        <header>
        <ul>
            <li class="header-text"><a class="header-link" href="../">Home</a></li>
            <li class="header-text"><a class="header-link" href="../mac0499">TCC</a></li>
            <li class="header-text header-text--clicked"><a class="header-link" href="">Blog</a></li>
        </ul>
        </header>

        <!-- Olá, você está lendo meu código horrível e deve ter notado que está tudo 
            hardcodado. Junte minha falta de vontade de fazer um servidorzinho pra mexer
            com AJAX e meu ódio ao Medium; e obtenha este blog mal-feito porém funcional.
            Viva a gambiarra :-) -->
            <!-- Hello, you're reading my horrible code and may have noticed that's all
            hardcoded. Put together my lack of will in building a mini-server to deal with
            AJAX and my hatred towards Medium: the result is this poorly made, yet
            functional blog. Long live the gambiarra :-) -->
        
        <div class="post">
            <h1>"Accountability e Transparência — O Direito a Explicações"</h1>
            <p class="subtitle">Resumos de alguns textos sobre o assunto e notas pessoais com o que compreendi em minha leitura.</p>
            <div class="author">Escrito por Daniela Favero em 21/10/2020</div>
            <img class="post-img" src="../img/code-projected-on-woman.jpeg"/>
            <p class="description">Imagem: Unsplash / @thisisengineering</p>

            <h2>Ananny M and Crawford K, ‘Seeing without Knowing: Limitations of the Transparency Ideal and Its Application to Algorithmic Accountability’. New Media & Society.</h2>
            <p>Este primeiro texto começa com uma contextualização histórica da transparência. Ele aponta o nascimento da valorização da transparência durante o Iluminismo (séculos XVII e XVIII) e como o conceito de transparência foi deixando de ser apenas performativo — sempre positivo, de modo a produzir entendimento — e passou a mostrar aspectos mais práticos. Alguns teóricos mencionavam a necessidade de um sistema de observação e conhecimento que prometeria uma forma de controle para obter plena transparência.</p>
            <p>Depois disso, surgem classificações tipológicas sobre o tema, dentre elas transparência <i>fuzzy</i> e <i>clear</i> (quanto ao nível de divulgação de informações), <i>accountability soft</i> e <i>hard</i> (a respeito de como órgãos transparentes devem responder por suas ações), transparência <i>upwards</i> e <i>downwards</i> (que diz respeito à hierarquia), transparência como processo e evento (sobre o formato), e, finalmente, transparência em retrospecto e tempo-real (considerando temporalidade).</p>
            <p>Ainda contextualizando, o texto traz à tona as importantes discussões atuais contidas em movimentos como hacktivismo e <i>open source</i>, além de organizações como a Wikipédia. Também é citada a preocupação com as possíveis discriminações algorítmicas em programas amplamente usados na sociedade.</p>
            <p>Toda essa discussão historicamente relevante sobre a transparência culmina numa definição de limites ideais para tal conceito. Os limites citados no texto são: a possibilidade da transparência estar desconectada do poder, a ameaça da privacidade, a obstrução intencional (quando há uma grande carga de informação que pode distrair o público do que realmente importa), criação de falsos binários, evocação de modelos neoliberais (valorizando a maximização do poder individual e minimização da interferência governamental), o fato de que transparência não necessariamente constrói confiança (questionando a ética do público que tem acesso às informações divulgadas), a possibilidade de se privilegiar o ver sobre o entender, as limitações técnicas e limitações temporais.</p>
            <p>O debate caminha para a questão: transparência é uma forma adequada de governar sistemas algorítmicos? Os autores do texto concluem que não, porque em contextos digitais, transparência não é só sobre revelar informações ou manter segredos; mas sobre continuamente manter plataformas, algoritmos e protocolos de <i>machine learning</i> que permitem visibilidade.</p>
            <p>Apesar de ser uma discussão meramente teórica, aqueles limites ideais encontram uso como ferramentas para guiar o modelo de transparência e <i>accountability</i> de quem administra sistemas algorítmicos. Além, conclui-se que transparência não é só sobre deixar o algoritmo e os dados utilizados visíveis, mas sim sobre ter responsabilidade sobre a composição do sistema; afinal, na prática, é mais importante ter uma visão através do sistema (examinando a relação entre as parte humanas e não-humanas) do que uma simples visão dentro dele.</p>
            <h3>Notas pessoais:</h3>
            <p>Esse texto é excelente e realmente me fez pensar sobre os limites da transparência e sobre como ela pode ser distorcida dependendo dos argumentos e intenções de quem a impõe. Deste modo, a transparência isolada não faz sentido além de ferramenta teórica para a imposição de limites (conforme citado no texto). É aí que surge a necessidade da <i>accountability</i>, a importância de se criar <i>softwares</i> responsáveis pelas decisões tomadas e que se debruçam sobre os impactos sociais do mesmo.</p>
        
            <h2>Burrell J., How the machine “thinks”: Understanding opacity in machine learning algorithms. Big Data & Society.</h2>
            <p>Este texto começa com a definição de opacidade: dados o <i>input</i> e o <i>output</i> não fica claro como foi feita uma classificação em particular. Além disso, nem sempre os <i>inputs</i> são totalmente conhecidos. Esse fenômeno ocorre com alta frequência em algoritmos de <i>machine learning</i>.</p>
            <p>A autora define três formas de opacidade: a intencional pela corporação ou por segredo de Estado, a falta de instrução técnica e a forma como algoritmos operam em escala de aplicação. A primeira é dita como essencial para a autoproteção desses órgãos, mas se revela cada mais nociva na prática com escândalos de abuso de privacidade. Esta seria facilmente resolvível com <i>open source</i>, permitindo que qualquer um (com o conhecimento técnico necessário) acesse o código fonte e compreenda as implementações.</p>
            <p>A falta de instrução técnica pode se resolver com educação acessível sobre algoritmos, dado que a cada dia se torna mais necessário conhecer o funcionamento destes para ser um cidadão esclarecido. A própria autora se utiliza de boa parte do texto para explicar o funcionamento de algoritmos de <i>machine learning</i> como redes neurais e SVM (<i>support vector machine</i>), cumprindo o papel do cientista de divulgar seu trabalho e tornar o conhecimento democrático.</p>
            <p>Finalmente, o problema da escalabilidade se dá pelo fato de que quanto mais dados de entrada, mais acurados ficam os sistemas de <i>machine learning</i>, mas também maior a complexidade dos mesmos. Apesar de haver uma certa intuição sobre como os dados se relacionam com os resultados em escalas menores, os classificadores (peça fundamental dos algoritmos de <i>machine learning</i>) se tornam cada vez menos interpretáveis conforme treinamos o programa com dados mais heterogêneos.</p>
            <p>As três sugestões que a autora dá para resolver este último problema são: parar de usar <i>machine learning</i> em contextos críticos (o que é um tanto extremado), se utilizar de <i>feature extraction</i> (remover os parâmetros do algoritmo que possam levar a algum tipo de discriminação) ou criar métricas para avaliar discriminação nas classificações que o programa fez.</p>
            <h3>Notas pessoais:</h3>
            <p>Na minha opinião, os dois primeiros problemas de transparência para algoritmos podem ser respectivamente resolvidos com <i>open source</i> e educação, bem como a própria autora menciona no texto. Uma nota sobre <i>open source</i>, é que tornar um sistema secreto não o torna mais seguro, segundo vários profissionais de segurança da informação (como pode se ler <a href="https://www.zdnet.com/article/six-open-source-security-myths-debunked-and-eight-real-challenges-to-consider/" target="_blank">aqui</a>). Não conhecia as possibilidades de se visualizar um sistema feito com <i>machine learning</i>, a explicação foi muito interessante e acredito que, mesmo que seja custoso computacionalmente, <i>softwares</i> que tomam decisões que impactarão na sociedade devem ser explicáveis.</p>

            <h2>Bibal A et al., ‘Legal Requirements on Explainability in Machine Learning’. Artificial Intelligence and Law.</h2>
            <p>Neste artigo, os autores começam contrapondo os conceitos de interpretabilidade — quando um modelo é compreensível por natureza, como árvores de decisão — e de explicabilidade — dado pela capacidade de explicar um modelo caixa-preta usando recursos externos, como a visualização.</p>
            <p>A partir disto, o artigo aborda os requerimentos legais que utilizarão esses conceitos, particularmente em relação à <i>General Data Protection Regulation</i> (GDPR) da União Europeia. Os requerimentos pediriam especificações sobre quem toma as decisões em um sistema e qual é o grau de automação no processo de tomada de decisão; afinal erros e vieses podem ter efeitos muito maiores na tomada de decisões feita por IA do que por humanos. Assim, esses requerimentos permitiriam que os afetados por uma decisão entendam sua justificativa e ajam de acordo, além de permitirem que a autoridade pública exerça controle efetivo significativo na legalidade da decisão.</p>
            <p>O texto ainda divide várias modalidades de requerimentos legais, em situações como B2B (<i>business-to-business</i>), B2C (<i>business-to-consumer</i>) — com suas respectivas leis horizontais. Também são cobertas regras setoriais, com legislações específicas para setores como o financeiro e o de seguro. Ainda, conclui-se que em relações G2C (<i>government-to-citizens</i>), a explicabilidade precisa ser ainda mais forte.</p>
            <p>Depois disso, constata-se que há diferentes níveis de explicabilidade, de modo que em alguns níveis, o requerimento é relativo ao modelo do sistema, enquanto que em outros é relativo à decisão tomada a partir do sistema.</p>
            <p>O primeiro nível é o dos requerimentos das <i>features</i> principais, que se dá a partir da obtenção dos parâmetros principais usados numa determinada decisão ou no modelo em si. O único porém deste nível é que o procedimento de <i>feature extraction</i>, que seria necessário para extrair os parâmetros mais relevantes do modelo, é computacionalmente difícil.</p>
            <p>O segundo nível é dos requerimentos de todas as <i>features</i> (ou todas que fazem parte do caminho tomado na árvore de decisão) processadas no modelo. O problema deste nível é que o tamanho do conjunto de <i>features</i> pode ser enorme, portanto muito complexo. Uma forma de resolver a alta complexidade dos dados, é fazer um trade-off entre acurácia e complexidade, dispondo aproximações boas o suficiente das <i>features</i> reais.</p>
            <p>Já o terceiro nível se dá pelos requerimentos da combinação de <i>features</i> numa decisão, o que é muito interessante porque oferece a completa explicação de decisões específicas do modelo (as justificativas em si). O problema é que para se conseguir essas combinações, é necessário que o modelo usado no sistema seja transparente (como árvores de decisão ou modelos lineares).</p>
            <p>Finalmente, o quarto nível é o dos requerimentos do modelo inteiro, desde que seja um modelo interpretável.</p>
            <p>Além disso, em casos de decisões administrativas, é necessário que os modelos forneçam os artigos da lei por trás de cada decisão. Neste contexto, uma situação descrita por fatos é fornecida como input para o modelo, juntamente com argumentos textuais de ambos os lados opostos. Deste modo, podem haver três tipos de dados de entrada para o modelo: fatos, artigos legais e argumentos. O texto descreve como se deve proceder em relação a cada possível <i>input</i>.</p>
            <h3>Notas pessoais:</h3>
            <p>As ideias apresentadas no texto são todas novas para mim e, ao meu ver, são juridicamente justas levando em conta análises mais teóricas e computacionais feitas nos dois textos anteriores (aliás esse diálogo entre os três textos é muito interessante, eles de fato se complementam nessas três áreas). Tive que pesquisar melhor sobre requerimentos e agora o conceito está um pouco mais claro.</p>

            <h2>FAT-ML Working Group,Principles for Accountable Algorithms and a Social Impact Statement for Algorithms.</h2>
            <p>Este texto é apenas uma lista de princípios para o desenvolvimento de algoritmos com <i>accountability</i>, e guias para aplicá-los. Os princípios são: responsabilidade (deve haver alguém responsável por revisar o sistema seus impactos sociais), explicabilidade (o modelo deve ser compreensível em termos não técnicos), acurácia (os dados devem ser acurados de modo a não causarem efeitos sociais negativos e, se não forem acurados, deve haver uma forma de mitigar as discrepâncias), auditabilidade (deve ser possível que outros possam revisar e questionar decisões do sistema, além de que devem haver termos de uso permissíveis) e justiça (garantir que não haja discriminação).</p>
            <p>No <i>"Social Impact Statement”</i>, o guia para aplicar esses princípios, o texto adiciona que para garantir que eles estejam sendo cumpridos, é necessário checá-los durante do estágio de <i>design</i>, pré-lançamento e pós-lançamento (este último tornando o <i>Statement</i> público). O texto ainda especifica exemplos para construir o <i>Social Impact Statement</i> para determinado sistema.</p>
            <h3>Notas pessoais:</h3>
            <p>Assim como do lado jurídico (apresentado no texto do A. Bibal), há o lado dos desenvolvedores de <i>software</i> para garantir <i>accountability</i> nos sistemas. O guia apresentado é bem sucinto e seus exemplos práticos clarificam como devem ser aplicados. Na minha opinião, o <i>Social Impact Statement</i> deveria ser uma exigência para <i>softwares</i> que tomarão decisões que impactarão na sociedade.</p>

            <h2>Bibliografia</h2>
            <ol>
                <li><span class=bib>Ananny M and Crawford K, <b>‘Seeing without Knowing: Limitations of the Transparency Ideal and Its Application to Algorithmic Accountability’.</b> New Media & Society. Disponível em: <a href="http://mike.ananny.org/papers/anannyCrawford_seeingWithoutKnowing_2016.pdf" target="_blank">http://mike.ananny.org/papers/anannyCrawford_seeingWithoutKnowing_2016.pdf</a></span></li>
                <li><span class=bib>Burrell J., <b>How the machine “thinks”: Understanding opacity in machine learning algorithms.</b> Big Data & Society.</span></li>
                <li><span class=bib>Bibal A et al., <b>Legal Requirements on Explainability in Machine Learning’.</b> Artificial Intelligence and Law. Disponível em: <a href="https://link.springer.com/article/10.1007/s10506-020-09270-4" target="_blank">https://link.springer.com/article/10.1007/s10506-020-09270-4</a></span></li>
                <li><span class=bib>AT-ML Working Group, <b>Principles for Accountable Algorithms and a Social Impact Statement for Algorithms.</b> Disponível em: <a href="http://www.fatml.org/resources/principles-for-accountable-algorithms" target="_blank">http://www.fatml.org/resources/principles-for-accountable-algorithms</a></span></li>
            </ol>

        </div>

        <footer>
            <ul>
                <li class="social">
                    <a href="http://github.com/danigfavero" target="_blank">
                        <img class="icon" src="../img/github.png" onmouseover="this.src='../img/github-orange.png';" onmouseout="this.src='../img/github.png';"/>
                    </a>
                </li>
                <li class="social">
                    <a href="http://linkedin.com/in/danigfavero" target="_blank">
                        <img class="icon" src="../img/linkedin.png" onmouseover="this.src='../img/linkedin-orange.png';" onmouseout="this.src='../img/linkedin.png';"/>
                    </a>
                </li>
                <li class="social">
                    <a href="http://lattes.cnpq.br/4380746598220062" target="_blank">
                        <img class="icon" src="../img/lattes.png" onmouseover="this.src='../img/lattes-orange.png';" onmouseout="this.src='../img/lattes.png';"/>
                    </a>
                </li>
            </ul>
            <p class="copyright">Feito com ♥ por Daniela Favero</p>
        </footer>
    </body>

</html>
